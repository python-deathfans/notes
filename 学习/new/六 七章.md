# 第六章 数据加载、存储、文件格式

## 6.1 读写文本格式的数据

![表6-1 pandas中的解析函数](http://upload-images.jianshu.io/upload_images/7178691-958f849e6067b19b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

由于csv文件以逗号分隔，所以我们可以使用**read_csv**将其读入一个**DataFrame**：

```python
In [9]: df = pd.read_csv('examples/ex1.csv')

In [10]: df
Out[10]: 
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo
```

也可以使用**read_table**，并指定分隔符：

```python
In [11]: pd.read_table('examples/ex1.csv', sep=',')
Out[11]: 
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo
```

并不是所有文件都有标题行。看看下面这个文件：

```python
In [12]: !cat examples/ex2.csv
1,2,3,4,hello
5,6,7,8,world
9,10,11,12,foo
```

读入该文件的办法有**两个**。你可以让pandas为其分配**默认的列名**，也可以**自己定义列名**：

```python
In [13]: pd.read_csv('examples/ex2.csv', header=None)
Out[13]: 
   0   1   2   3      4
0  1   2   3   4  hello
1  5   6   7   8  world
2  9  10  11  12    foo

In [14]: pd.read_csv('examples/ex2.csv', names=['a', 'b', 'c', 'd', 'message'])
Out[14]: 
   a   b   c   d message
0  1   2   3   4   hello
1  5   6   7   8   world
2  9  10  11  12     foo
```

# 第七章 处理缺失数据

![表7-1 NA处理方法](http://upload-images.jianshu.io/upload_images/7178691-1a0f73e5bb26ea21.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## dropna

+ 删除na所在的行
+ 默认丢弃任何含有NA的行或者列
+ **how参数**指定怎么删除,**how='all'**,全部是NA的时候才会去删除
+ axis=0 代表的是行，axis=1代表的是列
+ **thresh参数**代表的是删除NA之后剩余的元素的个数

## fillna

+ 填充缺失数据
+ fillna(
      **value**=None,
      **method**=None,
      **axis**=None,
      **inplace**=False,
      **limit**=None,
      **downcast**=None,
  ) 
  + 传入**字典**，对于**不同的列有不同的填充**
  + **method='fflill'**
    + 前向填充
  + **limit参数**代表填充的行数

## 移除重复数据

+ **duplicated()**
  + 返回**前面是否出现重复行**的布尔值
+ **drop_duplicates()**
  + 返回一个新的dataframe
  + 可以删除指定的列的重复值，传入单个columns

## 映射

+ **map**
  + 对于Series的函数
+ **apply**
  + 对于Dataframe类型的数据
  + 行或者列
+ **applymap**
  + 对于DataFrame类型中的每一个元素

## 替换值

利用**fillna方法填充缺失数据**可以看做值**替换的一种特殊情况**。前面已经看到，map可用于修改对象的数据子集，而replace则提供了一种实现该功能的更简单、更灵活的方式。

```python
data = pd.Series([1, -999, 2., -999, -1000])
data.replace(-999, NA)
```

+ 也可以一次传入多个值，替换多个
  + **data.replace([-999, -1000], NA)**
+ 要让**每个值有不同的替换值**，可以**传递一个替换列表**：

## 排列和随机采样

利用**numpy.random.permutation**函数可以轻松实现对Series或DataFrame的列的排列工作（permuting，随机重排序）。通过需要排列的轴的长度调用permutation，可产生一个表示新顺序的整数数组：

# 经济学

## 养老金

### 怎么交&存在哪里

+ 自己总资 8%
+ 公司交工资的 16%-20%
+ 钱存在了**社保基金**里，国家保管
+ 多年之后退休了，也是从这里领取
+ 计算养老金时，是以**当时的社会平均工资为基础的**，所以**不用担心钱贬值**的问题。
+ **正在工作的人缴钱，已经退休的人领钱**，这种玩法就叫**现收现付制**。

## 医疗保险

全名叫社会医疗保险，就是你向国家买了份保险，可以报销基础医药费的那种。

### 怎么交

+ 工资拿出2%
  + 交到医保卡里，这个叫**个人账户**
  + 平时头疼脑热，挂号开药，走的都是**个人账户**
+ 公司交8%
  + 小部分交到医保卡，大部分交到国家，**统筹账户**
  + **住院、大病**，动用**统筹账户**

### 怎么报销

所谓的报销，就是你看完病后，**医保给你补贴一些医药费**。有的是你在医院，**人家直接给报了**；有的比较麻烦一点，你得去当地管**社保**的地方跑一趟。

![](https://pic.downk.cc/item/5f3cc66c14195aa5948b1d4a.png)

地区不同，情况不同，**起付线和封顶线也不同**。起付线一般为**几百到一千多元**，封顶线一般为**几万到几十万元**。

**医保**必须要有，**商业保险**也要有，两者相互补充，共同保障安全。

## 一口气搞懂要交哪些税

税分很多种，我们几乎每天都在缴税

### 所得税

无论企业还是个人，赚了钱，都要把一部分上缴国家。

对于企业，可以减掉一部分费用，再交税：

+ **原料成本**
+ **人工成本**

纳税人

+ 居民纳税人
  + **常住中国**
  + 不管赚的什么币，都要交税
+ 非居民纳税人
  + **不常住中国**
  + 只有从中国赚的，才需要交税

**起征点**

正确叫法是**免征额**，这两个有明显的区别

+ **没有超过**这个点是**不需要纳税**的
+ **一旦过线**了，按照**全部工资**来交税

### 流转税

![](https://pic.downk.cc/item/5f3cc8ed14195aa5948c53a6.png)

当买瓜的时候，已经交了税了。

**商品在流转过程**交的税叫做**流转税**

+ **增值税**
  + **所有的商品**都需要交
+ **消费税**
  + 不是所有的
  + 奢饰品、高能耗、不可再生、特殊商品
+ **关税**
  + 国外进口的产品
  + 不仅可以让国家赚钱，还可以保护本土产业

### 财产资源税

人类资源

+ **先天资源**
  + 土地
+ **人造财产**
  + 房产

### 行为税

+ **印花税**
+ **购置税**

# 第十章 聚合与分组

![图10-1 分组聚合演示](http://upload-images.jianshu.io/upload_images/7178691-e5c671e09ecf94be.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

+ 总结来说,`groupby`的过程就是将原来的DataFrame按照groupby的字段，划分成若干个`分组DataFrame`,被分为多少个组就有多少个分组DataFrame.所以说，在`groupby之后的一系列操作`(如agg、apply等)，均是基于`子DataFrame`的操作。

### agg聚合操作

聚合操作是groupby后非常常见的一个操作，会写sql的朋友对此应该是非常熟悉了。聚合操作可以用来求和、均值、最大值、最小值等，下面给出常见的聚合操作：

| 函数  | 用途   |
| ----- | ------ |
| min   | 最小值 |
| max   | 最大值 |
| sum   | 求和   |
| mean  | 求平均 |
| count | 计数   |

![](https://pic.downk.cc/item/5f3e127b14195aa59471f706.png)

  # 合并&连接

## 轴连接

pd.concat()

+ Concatenate pandas objects along a `particular axis` with optional set logic
  along the other axes

+ ```python
  pd.concat(
      objs: Union[Iterable[Union[ForwardRef('DataFrame'), ForwardRef('Series')]], Mapping[Union[Hashable, NoneType], Union[ForwardRef('DataFrame'), ForwardRef('Series')]]],
      axis=0,
      join='outer',
      ignore_index: bool = False,
      keys=None,
      levels=None,
      names=None,
      verify_integrity: bool = False,
      sort: bool = False,
      copy: bool = True,
  ) -> Union[ForwardRef('DataFrame'), ForwardRef('Series')]
  ```

+ 默认是按行进行合并，如果想按照列合并，axis=1
+ `join`有多个选择，inner,outer,这里默认值是`outer`,下面会根据实例来比较下

## 合并

pd.merge()

```python
Signature:
pd.merge(
    left,
    right,
    how: str = 'inner',
    on=None,
    left_on=None,
    right_on=None,
    left_index: bool = False,
    right_index: bool = False,
    sort: bool = False,
    suffixes=('_x', '_y'),
    copy: bool = True,
    indicator: bool = False,
    validate=None,
) -> 'DataFrame'
```

+ `left`、`right`表示想要合并的dataframe
+  `how`,值可以是`inner,left,right,outer`，使用过mysql的人应该知道这是什么意思吧？就不过多翻译了，默认值是inner
+ `left_on`，`right_on`是用来指定`希望用来作用合并依据的列名`，如果不指定的话会`自动寻找列名相同的列`进行合并
+ `left_index`，`right_index`设置为True的话表示使用该DF的列索引作为合并的根据进行合并
+ sort默认为False，设置为True时表示合并时会根据给定的列值(也就是前面的left_on这种指定的列的值)来进行排序后再输出

# 各种连接方式

![](https://pic.downk.cc/item/5f3f356314195aa5949c45e2.png)

## 内连接

+ 利用内连接可获取两表的`公共部分的记录`

## 全连接

+ 公共部分记录集C＋表A记录集A1+表B记录集B1

## 左连接

+ 数据表A中的记录为`主循环体`，依次匹配数据表B中的记录，如果数据表A中连接字段Aid的值，在数据表B中没有Bnameid值与之对应，则**右**侧以`null`代替。

## 右连接

+  数据表B中的记录为`主循环体`，依次匹配数据表A中的记录，如果数据表B中连接字段Bnameid的值，在数据表A中没有Aid值与之对应，则**左**侧以`null`代替。

