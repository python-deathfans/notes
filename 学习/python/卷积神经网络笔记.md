[TOC]



![](C:\Users\包志龙\Desktop\常用\file\mark down笔记\图片\Snipaste_2020-01-14_05-11-32.png)



# 第一章 走进深度学习的世界

## 1.1 从人工智能到机器学习

+ **基本概念**
  + `AI`
    + 让机器去完成人类的智能工作
  + `强AI(通用AI)`
    + AI可以完成人类的全部智能工作
  + `CV`
    + 计算机视觉。AI中一大领域--理解图像
  + `NLP`
    + 自然语言处理。AI中的另一大领域--理解人类语言
  + NLP可以与CV结合，我们可要求AI用语言描述图像，看图说话，或根据图像回答人类的问题
+ **机器学习**是AI重要领域：
  + **让机器从训练数据中自动进步，从经验中自动学习**
  + 在机器学习外，AI还有很多方法。例如专家系统，是机器用人类定义的规则进行逻辑推理。这些都属于AI，但是不属于机器学习
+ **深度学习**是**机器学习**的重要领域：
  + 用深度神经网络完成机器学习
  + `深度学习是目前最热门的机器学习方法`，因为在许多问题上的效果最佳，尤其是在训练数据足够多的情况下
+ **强化学习**是**机器学习**的另一重要领域：
  + 让机器在环境中逐渐学会正确决策，最终获得最大利益
  + 难点是最终获取最大利益，在之前必须做出种种牺牲，因此不能只看眼前利益，需要建立长远的决策模型

# 第二章 深度卷积神经网络：第一课

**神经网络几大优势：**

+ 拥有极大的参数量，可以拟合极其复杂的数据。专业术语：`容量大`，可避免`欠拟合(under-fit)`
+ 运行速度快，`容易并行化`，在GPU上尤其快，在专用硬件（谷歌研发的TPU）上更快。换言之，我们可以快速运行和训练大规模的神经网络
+ 训练过程有快速而有效的方法：`反向传播`、`随机梯度下降`等

**超参数**

+ 调整超参数的过程，就是著名的调参
+ 包括学习速率，批大小，网络架构等
+ 设置合理的超参数，可以让网络达到更好的训练效果

**梯度消失 梯度爆炸**

+ 消失：传播过程中绝对值越来越小（直到变为0），会使得网络停滞不前
+ 爆炸：传播过程中绝对值越来越大（直到发散），使得网络不稳定，性能崩溃



# 第三章 卷积神经网络：第二课

## 3.1 重要理论知识

![](C:\Users\包志龙\Desktop\常用\file\mark down笔记\图片\Snipaste_2020-01-17_10-45-48.png)

### 3.1.1 数据：训练集、验证集、测试集

​	机器学习的第一步是收集数据。不过，当我们拥有大量的数据后，并不会直接将他们全部用于训练模型。这是因为我们训练网络的目标是希望它既能在现有数据上取得较佳的性能，又能在未来的新数据上取得较佳的性能。

+ 保证能学好**现有的数据**。往往需要调整一些**超参数**。如果学不好，成为欠拟合
+ 检验在新数据上的性能，称为**泛化能力**。
+ 最严格的做法是将数据分为三部分：`训练集用于训练网络`，`验证集用来调整网络`，`测试集`用于检测和预测网络对于新数据的效果。
+ **典型训练过程**：
  + 选定一组超参数，用训练集训练网络
  + 调整超参数，直到可在训练集上获得较好效果
  + 在验证集上运行网络，看效果如何。注意，**只运行网络，不能用验证集训练网络**
  + 调整超参数，直到可在训练集和验证集上都获得较好效果。
  + 最后，对于超参数满意后，在测试集上运行网络，得到的效果就代表了网络在全新数据上的性能。注意，**不能再根据网络在测试集的效果再修改超参数。**

### 3.1.2 常见分类

+ **有监督学习（数据带有标签值）**：
  + 回归
  + 分类
  + seq2seq

+ **无监督学习（数据没有标签值)**:
  + 聚类
  + 降维
  + 自编码
  + 推荐系统
  + 生成模型（编码器、生成对抗网络）

### 3.1.3 训练的障碍：欠拟合、过拟合

+ **欠拟合**两种原因
  + **网络的容量不足**。往往是网络的**广度或者深度不够**，可以通过增大网络的规模决定
  + 如果网络的**规模很大**，仍然拟合效果差，那就是因为**网络陷入局部的极值**，或者**网络的拟合速度不够**
    + 这可能是因为训练的**超参数不够妥当**，如学习速率过大或过小，**初始化方法不佳或者网络架构不适合**。可以通过相应的调整解决
    + 但是也有可能是**数据本身的问题**，如数据本身噪声过大、分布过于极端

## 3.2 神经网络的正则化

+ **主要是用来避免过拟合**
+ 通常在**神经元比较密集的时候加**

## 3.3 数据增强和预处理

+ **数据增强**
  + 改善过拟合的一种方法
  + 使得数据更加丰富，训练效果更好

+ **数据预处理**
  + 图像文件的原始像素介于0,255之间，如果直接输入到网络，很容易造成收敛速度很慢，一般是规范化到0,1或者-1,1之间
  + 白化
  + 直方图均衡

+ **BN（批规范化）层**，可以加快网络收敛，常被放置在`非线性激活层之前`，甚至可以放在每个非线性激活层之前。

# 第四章 卷积神经网络：第三课

### 残差网络：ResNet

#### 思想

+ ​	加入残差连接，相当于建立信息高速公路，然后深度网络的错误率即可随着网络的加深不断降低

+ 假设有一个网络M，输出是X
+ 给M加上一层变为M‘，令新的输出是H(X)
+ 有一种办法可以保证M'的性能至少不会比M差，那就是零新加入的一层为恒等变换，即直接令H(X)=X。于是M'和M的输出是一模一样。性能相同
+ 更好的办法是，令H(X)=x+F(x),然后只要求新的一层去学习F(x),这里的F(x)称为残差

