[TOC]



传统算法是让**机器执行** 

机器学习是让**机器学习**



## 回归任务

+ **定义：结果是一个连续的数字的值，而非一个类别**
+ 一些情况，回归任务可以简化成分类任务



## 机器学习的主要任务

+ 分类问题
+ 回归问题

![机器学习](C:\Users\包志龙\Desktop\常用\file\mark down笔记\图片\Snipaste_2019-11-03_08-59-44.png "机器学习")



## 机器学习算法分类

+ ***监督学习***
  + 给机器标记信息，**含有标签值**，有训练目标
  + **分类问题**
  + **回归问题**
  + 算法：
    + k近邻
    + 线性回归和多项式回归
    + 逻辑回归
    + SVM
    + 决策树和随机森林
  
+ ***非监督学习***
  + 给机器训练的数据**没有任何“标记”和“标签”**
  + **聚类分析**
  + **降维处理**
  + **异常检测**
  
+ ***半监督学习***
  + 一部分数据有标记，一部分没有标记
  + 一般先使用无监督学习进行训练，然后再使用监督学习进行训练
  
+ ***增强学习***
  
  + 根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式
  
+ ***批量学习 batch learning***
  + 优点：简单
  + 问题：如何适应环境变化
    + 解决方案：定时重新批量学习
  + 缺点：每次批量学习，运算量巨大
  + 在某些变化较快的情况下，学习情况没法正常进行下去
  
+ ***在线学习  online learning***
  + 优点：及时反映新的环境变化
  + 问题：新的数据带来不好的变化？
  + 解决方案：需要加强对数据进行监控
  
+ ***参数学习***

+ ***非参数学习***

  

## 数据加载和简单的数据探索

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

# 类型是字典，通过键值对进行访问
iris = datasets.load_iris()

X = iris.data[:, 2:]   # 真正的数据
target = iris.target    # 标签值

plt.scatter(X[target == 0, 0], X[target == 0, 1], marker="o")
plt.scatter(X[target == 1, 0], X[target == 1, 1], marker="*")
plt.scatter(X[target == 2, 0], X[target == 2, 1], marker="+")

plt.xticks(())
plt.yticks(())

plt.show()
```



##  KNN算法--K近邻算法

	+ **取一个k值**
	+ **在所有的点中寻找与新添加的点距离最近的k个点**
	+ **然后看，这k个样本中哪个样本较多，新加的点就属于哪个样本**





![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-04_19-47-19.png "欧式距离")

![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-05_21-17-56.png "曼哈顿距离")



![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-05_21-18-59.png "曼哈顿距离和欧式距离")



![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-05_21-20-01.png "明可夫斯基距离")

**手写KNN算法**

```python
import numpy as np
import matplotlib.pyplot as plt
from math import sqrt
from collections import Counter  # 计算频次的函数，返回每个元素出现的频次


raw_data_X = [
    [3.393, 2.331],
    [3.11, 1.781],
    [1.343, 3.368],
    [3.582, 4.679],
    [2.28, 2.869],
    [7.423, 4.696],
    [5.745, 3.533],
    [9.172, 2.511],
    [7.792, 3.424],
    [7.939, 0.791]
]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)
x = np.array([8.093, 3.365])

plt.scatter(X_train[y_train == 0, 0], X_train[y_train == 0, 1], color="g")
plt.scatter(X_train[y_train == 1, 0], X_train[y_train == 1, 1], color="r")
plt.scatter(x[0], x[1], color="b")
plt.show()

distances = [sqrt(np.sum((x_train-x) ** 2)) for x_train in X_train]
# print(distances)

k = 6

nearest = np.argsort(distances)  # 从小到大排序，返回的是下标

topK_y = [y_train[i] for i in nearest[:k]]

votes = Counter(topK_y)
predict_y = votes.most_common(1)[0][0]
print(predict_y)
```



**调用sklearn库使用KNN算法**

```python
from sklearn.neighbors import KNeighborsClassifier
import numpy as np

KNNClassifier = KNeighborsClassifier(n_neighbors=6)

raw_data_X = [
    [3.393, 2.331],
    [3.11, 1.781],
    [1.343, 3.368],
    [3.582, 4.679],
    [2.28, 2.869],
    [7.423, 4.696],
    [5.745, 3.533],
    [9.172, 2.511],
    [7.792, 3.424],
    [7.939, 0.791]
]
raw_data_y = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]

X_train = np.array(raw_data_X)
y_train = np.array(raw_data_y)
x = np.array([8.093, 3.365]).reshape(1, -1)

KNNClassifier.fit(X_train, y_train)

# x 必须是二维数组
predict_y = KNNClassifier.predict(x)[0]
print(predict_y)
```



## 判断机器学习算法的性能



![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-04_20-48-05.png "性能判断")



		+ 训练和测试数据分离
		+ 训练数据训练模型
		+ 测试数据测试模型



![](C:\Users\包志龙\Desktop\常用\file\日常学习\神经网络学习\Sklearn\Snipaste_2019-11-04_20-50-14.png)



## 超参数和模型参数

 + 超参数：在算法运行前需要决定的参数
   	+ 用循环进行过滤
      	+ 也就是调参的过程
      	+ **如果寻找的最好的参数在边界，需要重新调整边界**
	+ 模型参数：算法过程中学习的参数



