## LeNet-5(1998)

![](https://pic.downk.cc/item/5ea6cb08c2a9a83be5abe38d.png)



+ 算上输入输出是7层
+ 不算的话是5层

各层参数详解

+ 输入层
  + 32*32
  + 注：传统上，**不将输入层视为网络层次结构**
+ C1层-卷积层
  + 输入:32x32
  + 卷积核:5x5
  + 卷积核个数:6
  + 输出feature map:(32-5+1)*(32-5+1)=28x28
  + 可训练参数:(5x5+1)x6 
+ S2层-池化层
  + 输入:28x28
  + 采样区域:2x2
  + 输出feature map:14x14*6

+ C3层-卷积层
  + 输入:14x14x6
  + 卷积核:5x5
  + 卷积核个数:16
  + 输出feature map:10x10x16

+ S4层-池化层
  + 输入:10x10x16
  + 输出feature map:5x5x16
+ C5层

## AlexNet(2012)

<img src="https://pic.downk.cc/item/5ea6ce4ac2a9a83be5ae9201.jpg" style="zoom:50%;" />

> 网络一共有**8层可学习层**——**5层卷积层**和**3层全连接层**。网络的输入为150,528（224×224×3）
> 维，各层的神经元数量为：253,440=>186,624=>64,896=>64,896=>43,264=>4096=>4096=>1000(ImageNet有
> 1000个类）

网络与之前的网络相比引入了以下新特征:

+ **数据预处理**，将图像随机剪切，并随机翻转
+ **更深的网络结构**
+ **使用层叠的卷积层**，即卷积层+卷积层+池化层提取图像特征
+ **ReLU**非线性单元
  + 激活函数的作用是为了**引入非线性**，这么说很抽象，但是在没有激活函数的时候，无论有多少层MLP，得到的网络输出依然是关于输入的线性函数
  + **传统的激活函数一般 sigmoid tanh**两种**饱和**非线性函数，就训练时间来说，使用这些饱和的非线性函数会比使用非饱和的非线性函数ReLU，模型**收敛需要更长的时间**。而对于大型的数据集来说，更快的学习过程意味着可以节省更多的时间。除此之外，ReLU也引入了一定的**稀疏性**，在特征表示的范畴内，数据有一定的稀疏性，也就是说，有一部分的数据其实是冗余的。通过引入ReLU，可以模拟这种稀疏性，以最大近似的方式来保留数据的特征。
+ **多GPU训练**
  + 在论文给出的图中我们可以看到，两个网络是并行的，但是这并不意味着AlexNet一定是并行结构的，这张图只是告诉我们，将AlexNet部署在多GPU上时的工作方式（为什么要部署在多GPU上？因为当时的GTX580只有3GB的显存，不足以容纳该网络最大的模型）：每个GPU上并行地、分别地运行AlexNet的一部分（例如将4096个神经元的全连接层拆分为两个并行的2048个神经元的全连接层，第一个卷积层有96个feature map而在一块GPU上只有48个feature map等），两块GPU只在特定的层上有交互。
+ 采用随机丢弃策略(**dropout**)
+ **局部响应归一化**(LRN)
+ **重叠的池化**(步长小于卷积核的尺寸)
+ ![](https://pic.downk.cc/item/5ed38688c2a9a83be547587a.jpg)

## VGGNet(2014)

![](https://pic.downk.cc/item/5ea6d134c2a9a83be5b15c28.jpg)

> VGGNet与AlexNet很相似，都是卷积池化-卷积池化-……全连接的套路，不同的是**kernel大小**，卷积**stride**，**网络深度**。VGGNet将**小卷积核**带入人们的视线，分析一下大小卷积核的区别与优劣：
> 在上面提到的AlexNet中第一个卷积层使用的kernel大小为**11×11**，stride为**4**，C3和C5层中使用的都是**5×5**的卷积核；而出现在VGGNet中大多数的卷积核都是大小为**3×3**，stride为**1**的。

5x5 = 2个3x3

7x7 = 3个3x3

+ 小卷积核带来的好处
  + **减少了参数个数**
    + 两个串联的小卷积核需要3×3×2=18个参数，一个5×5的卷积核则有25个参数。
      三个串联的小卷积核需要3×3×3=27个参数，一个7×7的卷积核则有49个参数。
      大大减少了参数的数量。
  + **引入了更多的非线性**
    + 多少个串联的小卷积核就对应着多少次激活（activation)的过程，而一个大的卷积核就只有一次激活的过程。引入了**更多的非线性变换**，也就意味着**模型的表达能力会更强**，可以去拟合更高维的分布。
    + VGGNet相比于AlexNet**层数更深**，**参数更多**，但是却可以更快的收敛，在网上被解释为“更深的网络层数和更小的卷积核起到了**隐式的正则化效果**”。

## GoogleNet(2014)

> Google公司的Christian Szegedy等人在**2014**年提出了一个网络结构GoogLeNet，主要由一种称为**Inception**的模块组成。它在不增加计算资源的前提下**提高了网络的深度**，从而达到提高预测性能的目的。
>
> ![](https://pic.downk.cc/item/5ed38738c2a9a83be54841c8.jpg)

GoogLeNet的Inception模块将**多个尺度特征融合**，起到了非常显著的效果，灵感主
要来自**Serre的多尺度模型**，它使用了不同尺度Gabor过滤器来处理不同尺度的图片。
Inception 使用了类似的策略，但是不同于它固定两层深度模型，所有Inception 模块结构中的过滤器都是不断被学习更新的。此外，Inception 模块在GoogLeNet中被重复了很多次，**9个Inception模块**以及其他的一些卷积层共同组成了22层的深度卷积神经网络。
**Network-In-Network**为了增加网络表达能力，使用了**1x1的卷积层**。GoogLeNet也
使用了这种结构，除了增加网络的表达能力，**主要还在于降维**，以此来消除网络的瓶颈，否则这个瓶颈会限制网络大小。在使用了这种小卷积之后，不仅可以增加网络的深度，而且可以增加宽度。这种1x1的卷积虽然能够大大降低计算量，但是1x1卷积的感受野太小，很难获取到全局的信息，所以同时使用了3x3和5*5的卷积。

+ **Inception-v2**
  + 在v1的基础上加入**batch normalization**技术，在tensorflow中，使用BN在激活函数之前效果更好；将**5×5卷积替换成两个连续的3×3卷积**，使网络更深，参数更少
+ **Inception-v3**
  + 核心思想是将卷积核分解成更小的卷积，如将**7×7分解成1×7和7×1**两个卷积核，使网络参数减少，深度加深
+ **Inception-v4**
  + **引入了ResNet**，使训练加速，性能提升。但是当滤波器的数目过大（>1000）时，训练很不稳定，可以加入**activate scaling**因子来缓解