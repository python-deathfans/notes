## LeNet-5(1998)

![](https://pic.downk.cc/item/5ea6cb08c2a9a83be5abe38d.png)



+ 算上输入输出是7层
+ 不算的话是5层

各层参数详解

+ 输入层
  + 32*32
  + 注：传统上，**不将输入层视为网络层次结构**
+ C1层-卷积层
  + 输入:32x32
  + 卷积核:5x5
  + 卷积核个数:6
  + 输出feature map:(32-5+1)*(32-5+1)=28x28
  + 可训练参数:(5x5+1)x6 
+ S2层-池化层
  + 输入:28x28
  + 采样区域:2x2
  + 输出feature map:14x14*6

+ C3层-卷积层
  + 输入:14x14x6
  + 卷积核:5x5
  + 卷积核个数:16
  + 输出feature map:10x10x16

+ S4层-池化层
  + 输入:10x10x16
  + 输出feature map:5x5x16
+ C5层

## AlexNet(2012)

<img src="https://pic.downk.cc/item/5ea6ce4ac2a9a83be5ae9201.jpg" style="zoom:50%;" />

> 网络一共有**8层可学习层**——**5层卷积层**和**3层全连接层**。网络的输入为150,528（224×224×3）
> 维，各层的神经元数量为：253,440=>186,624=>64,896=>64,896=>43,264=>4096=>4096=>1000(ImageNet有
> 1000个类）

网络与之前的网络相比引入了以下新特征:

+ **ReLU**非线性单元
  + 激活函数的作用是为了**引入非线性**，这么说很抽象，但是在没有激活函数的时候，无论有多少层MLP，得到的网络输出依然是关于输入的线性函数
  + **传统的激活函数一般 sigmoid tanh**两种**饱和**非线性函数，就训练时间来说，使用这些饱和的非线性函数会比使用非饱和的非线性函数ReLU，模型**收敛需要更长的时间**。而对于大型的数据集来说，更快的学习过程意味着可以节省更多的时间。除此之外，ReLU也引入了一定的**稀疏性**，在特征表示的范畴内，数据有一定的稀疏性，也就是说，有一部分的数据其实是冗余的。通过引入ReLU，可以模拟这种稀疏性，以最大近似的方式来保留数据的特征。
+ **多GPU训练**
  + 在论文给出的图中我们可以看到，两个网络是并行的，但是这并不意味着AlexNet一定是并行结构的，这张图只是告诉我们，将AlexNet部署在多GPU上时的工作方式（为什么要部署在多GPU上？因为当时的GTX580只有3GB的显存，不足以容纳该网络最大的模型）：每个GPU上并行地、分别地运行AlexNet的一部分（例如将4096个神经元的全连接层拆分为两个并行的2048个神经元的全连接层，第一个卷积层有96个feature map而在一块GPU上只有48个feature map等），两块GPU只在特定的层上有交互。

## VGGNet(2014)

![](https://pic.downk.cc/item/5ea6d134c2a9a83be5b15c28.jpg)

> VGGNet与AlexNet很相似，都是卷积池化-卷积池化-……全连接的套路，不同的是**kernel大小**，卷
> 积**stride**，**网络深度**。VGGNet将**小卷积核**带入人们的视线，分析一下大小卷积核的区别与优劣：
> 在上面提到的AlexNet中第一个卷积层使用的kernel大小为**11×11**，stride为**4**，C3和C5层中
> 使用的都是**5×5**的卷积核；而出现在VGGNet中大多数的卷积核都是大小为**3×3**，stride为
> **1**的。

5x5 = 2个3x3

7x7 = 3个3x3

+ 小卷积核带来的好处
  + **减少了参数个数**
    + 两个串联的小卷积核需要3×3×2=18个参数，一个5×5的卷积核则有25个参数。
      三个串联的小卷积核需要3×3×3=27个参数，一个7×7的卷积核则有49个参数。
      大大减少了参数的数量。
  + **引入了更多的非线性**
    + 多少个串联的小卷积核就对应着多少次激活（activation)的过程，而一个大的卷积核就只有一次激
      活的过程。引入了**更多的非线性变换**，也就意味着**模型的表达能力会更强**，可以去拟合更高维的分
      布。
    + VGGNet相比于AlexNet**层数更深**，**参数更多**，但是却可以更快的收敛，在网上被解释为“更深的
      网络层数和更小的卷积核起到了**隐式的正则化效果**”。

