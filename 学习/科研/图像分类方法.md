## 传统方法

+ 两个阶段
  + 首先对图像进行特征提取
    + 使用若干**特征描述符**对图像进行**标记**，即将原始图像的**非结构化数据**转化为用以表示特征的**结构化数据**
  + 将这些结构化特征数据输入至可训练的**分类器**，得到分类结果。
+ 问题
  + 是**分类准确度**取决于**提取特征的有效性**

## 现代方法

+ **CNN-卷积神经网络**
  + 只需要通过调节权重和偏置以及优化器等参数，就可以进行特征的精确提取
  + 不需要依赖其他的数据

## CNN-卷积神经网络

+ 基本结构
  + 卷积层
  + 池化层
  + 全连接层
  + 输出层
+ 工作原理
  + 每层由多个**二维向量**组成，每个向量由多个**独立神经元**组成。每个二维向量可以视为**一张图像**，每一张图像均具有很强的**空间相关性**，而使用图像中独立像素作为输入将破坏这些相关性。因此，将输入图像通过多个不同的卷积核进行卷积运算，并加上偏置，提取出**局部特征图**，每个卷积核映射出一个新的二维图。将卷积运算的输出结果进行**非线性激活函数**处理，对激活函数的结果再进行**池化操作**，**保留最显著**的特征，提升模型的畸变容忍能力。经过多个卷积层和池化层后，将获取的显著特征通过全连接层，最后利用数理统计的方法或者分类器，输出得到相应的结果。

## 优化方法

### 基于网络结构优化的方法

+ **层结构优化**
  + 在对CNN每一层神经元规模进行调整时，通常是通过增加与上一层进行运
    算的卷积核来实现的，一般情况下**增加层的规模**可以增加每一层网络对于输入图
    像的特征表达，有利于提升网络模型对训练集的**拟合程度**，进而提升图像分类的
    准确度。
+ **增加网络的深度**
  + 调整**卷积核的尺寸和步长**。这种方法多用于处理较**大尺寸**的图像，在**第一个卷积层**实现较大程度的尺寸变化，以降低网络模型的计算量，减轻硬件负担，这就给调整网络模型的深度带来了空间。而对于形如24×24像素尺寸的图像，如果按照标准的CNN结构，网络模型的层数基本上是固定的，很少使用调整卷积核尺寸和步长的方法来增加网络深度
  + **增加卷积层或处理模块**。Alexnet网络模型中，在第3层和第4层叠加使用了两个13×13×192的卷积层。VGGnet的层结构描述图中可以看出，VGGnet不同的版本之间，主要的差异就是叠加使用了不同数量的卷积层，这种方式的叠加使用虽然提升了网络性能，但也大大增加了网络的训练量。
+ **参数优化方法**
  + **卷积核优化**
    + **大尺寸的卷积核**可以带来**更大的局部感受野**，但也需要训练**更多的参数**。很多CNN常使用**5×5**尺寸的卷积核，在为了大幅降低图像尺寸的卷积运算中，也会选用更大尺寸的卷积核，并配合大于1的步长使用。大卷积核可以使用**多个小**
      **卷积核替代**，起到**降低参数量**的效果。其中**5×5**卷积核可以分解成**两个3×3**的组合，而**7×7**可以分解成5×5和
      3×3的组合，或者**三个3×3**的组合。参数量实现了大幅的下降，但依然实现了原有的卷积运算。
    + ![](https://pic.downk.cc/item/5e9edfb4c2a9a83be59cda8e.png)
  + **卷积层通道优化**
    + 使用**1×1卷积核**对卷积层通道数实现**升维**和**降维**，**减少训练参数量**。假定输入为64通道的24×24的图像，通过3×3的卷积核实现输出同样为64通道的24×24的图像。而对于卷
      积层通道优化时，首先使用1×1卷积核对输入降维，进行3×3的卷积运算后，再使用l×1卷积核升维，实现64通道的24×24的图像输出。对比两组模型的参数量，优化后的模型减少了近**60%**的参数量。
    + ![](https://pic.downk.cc/item/5e9ee2cec2a9a83be5a01526.png)

### 基于数据集的优化方法

+ `数据增强`
  + **放射变换**
    + 平移、翻转、旋转
  + **尺度变换**
    + 放大、缩小、裁切或模糊度变换
  + **对比度变换**
  + **噪声扰动**

## 解决过拟合的方法

### 正则化

+ 采用正则化方法会**削弱不重要的特征变量**，自动从许多的特征变量中**提取重要的特征变量**，减小特征变量的数量级。

### 网络稀疏性优化

+ 过拟合问题是由于网络模型在训练的过程中**过度依赖**了图中圈出的几个**特征点**，导致了模型**过度的拟合了噪声点**，而忽略了数据的整体分布。一般情况下，过拟合问题很容易出现在**全连接**的神经网络中，这也是卷积神经网络优于传统人工神经网络的主要特征，因此对网络进行稀疏性优化有利于降低过拟合。对于**CNN模型**，过拟合更容易出现**全连接层**，网络的稀疏性优化也以优化全连接层为主。
  **Hinton** 等人（2012）[16]提出了随机失活（**dropout**)方法，指在神经网络网络的训练过程中，输入层的神经元按照一定概率**随机的将其暂时从网络中丢弃**，以达到改变网络结构的目的，相当于对完整的神经网络抽样出一些子集，每次只更新子网络的参数，在训练中隐藏了部分神经元，对网络模型进行了“**瘦身**”。

### 层数据分布优化

+ 过拟合问题主要表现为网络模型的**泛化能力差**，虽然对训练的数据进行了充分的学习，有较好的拟合效果，但对未知数据的预测能力较差。这也可以看做由于数据集中的每个**样本数据分布的差异较大**，导致网络模型在训练和测试的过程中需要不断适应新的数据分布，而影响了模型的泛化能力。在进行网络模型的训时，输入图像之间、层与层之间图像的数据分布均存在差异，减少这种差异可以
  消除权值更新的局部波动，起到减少过拟合的效果。
  **批规范化**（**Batch Normalization**)算法就是用做将数据分布进行归一化的方法，即是将每一层的输入数据归一化至**均值为0、方差为1**的分布，以降低前层输入数据分布的显著变化对后层输入数据分布的影响。在实际应用中，表现为在网络的每一层的数据输入**激活函数前**，插入一个BN归一化层，对数据进行归一化处理。

## 迁移学习

+ 定义
  + 迁移学习（Transfer Learning）是将一个分类问题上**训练好的模型**通过**简单的调整**使其适用于**新的分类问题**的方法。
  + <img src="https://pic.downk.cc/item/5e9ef175c2a9a83be5ae8676.png" style="zoom:67%;" />
+ 方法
  + 在训练CNN模型时，往往会**间隔一定的步数**将网络的**参数保存起来**，以便后期验证训练效果。这些网络模型的权值参数
    便可以用作迁移学习的初始参数，一般情况下，将训练好的CNN模型用于新的分类问题时，**保留卷积层和池化层**的所有参数，如图4.17中框出的部分，仅对**全连接层**中的参数进行训练。由于图中的模型已经在相应的数据集上验证了可以达到较好的分类准确度，可以将图中框出部分视为对图像进行特征提取的过程，其输出为一个更加精简且具有**更强表达能力**的特征向量，而**全连接层类似于传统的分类器**。
  + ![](https://pic.downk.cc/item/5e9ee89fc2a9a83be5a72c58.png)
+ 策略分析
  + 迁移学习的策略有很多，重要的因素有两条：一是**新数据集和原数据集的相似程度**，二是**新数据集的大小**。下面对做迁移学习时的四种情况做具体分析：
    + 相比于原数据集，**新数据集规模更小**，在内容上**相似**，即它们的数据分布比较相似。数据规模太小，对整个CNN网络都进行微调容易产生过拟合。**浅层网络**主要是提取**低级语义特征**，可以直接共享；高层网络提取更高级的语义，但是新旧数据很相似，所以不需要针对性地学习不同参数。因此，可以将特征层的参数**全部迁移过来**，并且固定参数不训练，**只训练分类器**，或者适当地训练一部分较高层的网络。
    + 相比于原数据集，**新数据集规模更大**，在内容上**相似**。由于收集了较多的数据，即使**微调整个网络**也不会产生过拟合。
    + 新数据集不仅**规模更小**，而且与原数据集**很不相似**。这种情况下预训练模型包含了更多的原数据集特定特征，再加上新数据集规模不够大，在一个**不太深的网络**上只训练一个分类器可能更好。
    + 新数据集**规模较大**，在内容上与原数据集**很不相似**。因为新数据足够多，且与原数据集不太相像，那么预训练模型的浅层网络也可能会在新数据集上不太适用，所以**从头开始训练一个模型会比在预训练模型上微调更好**。然而，在实践过程中，在ImageNet等大型数据集上训练的模型往往**能够适用于新数据集**，经过微调可以达到更好的性能。
+ **最佳微调深度分析**
  + 我们发现，最高的准确率首先会随着微调层数的加深而逐渐升高，然后在某个深度上达到最高，再逐渐递减，呈一个**单峰状的曲线**。

## 自适应微调深度迁移学习

